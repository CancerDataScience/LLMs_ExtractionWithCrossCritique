{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":7730726,"sourceType":"datasetVersion","datasetId":4517294},{"sourceId":7730755,"sourceType":"datasetVersion","datasetId":4517365},{"sourceId":7908951,"sourceType":"datasetVersion","datasetId":4646095},{"sourceId":7909031,"sourceType":"datasetVersion","datasetId":4069918}],"dockerImageVersionId":30664,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Installation of required libraries\n# You need pip version compatiable with your Python version\n\n# In case if you need to install missing libraries,\n# Create a cell, and write \"!pip install library_name\"","metadata":{"_uuid":"0e3bfe06-bf27-4028-a604-fd3e341a6d08","_cell_guid":"360abcc2-b0c1-4f78-b674-891659e57cbe","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install PyMuPDF","metadata":{"_uuid":"d7e5a2d3-eddc-4b67-a605-2988f6da4199","_cell_guid":"e2011520-3914-42d7-b162-2b573664d667","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install openai","metadata":{"_uuid":"20d330b2-ecfc-4d06-9186-90d67037564b","_cell_guid":"4d52845e-a1e1-4f22-8124-81b1e7e22d38","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install tiktoken","metadata":{"_uuid":"35453ad2-ab1b-4659-8a7f-a0ae5ee727f6","_cell_guid":"8e8046c8-f997-4d84-8c95-5f00fffb7bbe","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install anthropic","metadata":{"_uuid":"c9f7ab9f-15f7-47f3-a1bd-3f394909a0de","_cell_guid":"3ab4dd7c-5364-4d3c-9ffc-20dab3b42b79","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import the required libraries\nimport fitz\nimport tiktoken\n\nimport numpy as np\nimport pandas as pd\n\nimport openai\nfrom openai import OpenAI\nfrom dotenv import dotenv_values\nimport anthropic\n\nimport base64\nimport requests\nimport math, random\nimport os, time, sys\nimport re, string","metadata":{"_uuid":"94d3a43d-f958-4a88-979c-e5479e713a9f","_cell_guid":"dc622e63-6e33-47bf-ab43-ec82813891d5","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Text Module","metadata":{"_uuid":"f0db302b-9679-4fe0-8996-fc890be5324c","_cell_guid":"84e7a4ce-3c2d-4912-a8ef-d44a1ae44431","trusted":true}},{"cell_type":"code","source":"def extract_text_from_pdf(pdf_path): # This function will accept the .pdf file path\n    \n    pdf_document = fitz.open(pdf_path) # open the file using fitz library\n    extracted_text = \"\" # extracted text will be added to this storage variable\n    \n    # Iterate over each page\n    # pdf_document.page_count is a built-in function that count the number of pages\n    for page_number in range(pdf_document.page_count):\n        page = pdf_document[page_number] # Here, we can object named page\n        page_text = page.get_text() # Here, the text has been extracted from the page\n        # Append the text to the overall extracted_text string\n        extracted_text += page_text # += append mode\n        \n    print (\"Original Text:\",len(extracted_text))\n    \n    # Exclude the irrelevant sections\n    \n    last_section_start = -1\n    last_section_start = extracted_text.find(\"REFERENCES\\n\")\n    if last_section_start == -1:\n        last_section_start = extracted_text.find(\"References\\n\")\n    if last_section_start == -1:\n        last_section_start = extracted_text.find(\"References \\n\")\n    if last_section_start != -1:\n        extracted_text = extracted_text[:last_section_start].strip()\n    \n    last_section_start = -1\n    last_section_start = extracted_text.find(\"ORCID\\n\")\n    if last_section_start != -1:\n        temp_text = extracted_text[last_section_start:-1]\n        table_check = temp_text.find(\"TABLE\")\n        if table_check == -1:\n            extracted_text = extracted_text[:last_section_start].strip()\n        else:\n            table_check = -1\n    \n    last_section_start = -1\n    last_section_start = extracted_text.find(\"Acknowledgements\\n\")\n    if last_section_start == -1:\n        last_section_start = extracted_text.find(\"ACKNOWLEDGEMENTS\\n\")\n    if last_section_start != -1:\n        temp_text = extracted_text[last_section_start:-1]\n        table_check = temp_text.find(\"TABLE\")\n        if table_check == -1:\n            extracted_text = extracted_text[:last_section_start].strip()\n        else:\n            table_check = -1\n        \n    last_section_start = -1\n    last_section_start = extracted_text.find(\"Appendix\\n\")\n    if last_section_start == -1:\n        last_section_start = extracted_text.find(\"APPENDIX\\n\")\n    if last_section_start != -1:\n        temp_text = extracted_text[last_section_start:-1]\n        table_check = temp_text.find(\"TABLE\")\n        if table_check == -1:\n            extracted_text = extracted_text[:last_section_start].strip()\n        else:\n            table_check = -1        \n    \n    last_section_start = -1\n    last_section_start = extracted_text.find(\"Contributors\\n\")\n    if last_section_start == -1:\n        last_section_start = extracted_text.find(\"AUTHOR CONTRIBUTIONS\\n\")\n    if last_section_start == -1:\n        last_section_start = extracted_text.find(\"AUTHOR CONTRIBUTION\\n\")\n    if last_section_start != -1:\n        temp_text = extracted_text[last_section_start:-1]\n        table_check = temp_text.find(\"TABLE\")\n        if table_check == -1:\n            extracted_text = extracted_text[:last_section_start].strip()\n        else:\n            table_check = -1\n    \n    last_section_start = -1\n    last_section_start = extracted_text.find(\"AFFILIATIONS\\n\")\n    if last_section_start == -1:\n        last_section_start = extracted_text.find(\"Affiliations\\n\")\n    if last_section_start != -1:\n        temp_text = extracted_text[last_section_start:-1]\n        table_check = temp_text.find(\"TABLE\")\n        if table_check == -1:\n            extracted_text = extracted_text[:last_section_start].strip()\n        else:\n            table_check = -1\n    \n    last_section_start = -1\n    last_section_start = extracted_text.find(\"Declaration of interests\\n\")\n    if last_section_start == -1:\n        last_section_start = extracted_text.find(\"DECLARATION OF INTERESTS\\n\")\n    if last_section_start != -1:\n        temp_text = extracted_text[last_section_start:-1]\n        table_check = temp_text.find(\"TABLE\")\n        if table_check == -1:\n            extracted_text = extracted_text[:last_section_start].strip()\n        else:\n            table_check = -1\n    \n    print (\"Extracted Text:\",len(extracted_text))\n\n    # Close the PDF file\n    pdf_document.close()\n    return extracted_text\n\ndef get_tokens(string: str, encoding_name: str, ch_start, ch_end) -> int:\n    \n    encoding = tiktoken.get_encoding(encoding_name)\n    tokens = encoding.encode(string) # encode the text into tokens\n    tokens_str = [encoding.decode_single_token_bytes(token) for token in tokens]\n    chunks = [list(np.arange(a, a + ch_start)) for a in range(0, len(tokens_str), ch_end)]\n    \n    list_of_strings = []\n    for c in range(len(chunks)):\n        start = chunks[c][0]\n        end = chunks[c][-1]\n        mystring = encoding.decode(tokens[start: end]) # decode to form string for each chunk\n        list_of_strings.append(mystring) # text/strings of all chunks is mapped in the list\n        \n    return list_of_strings\n\n# Processing of Text by GPT\ndef gpt_function(t, p, gpt_key, model_name):\n    \n    client = OpenAI(\n        api_key = gpt_key,\n    )\n    \n    chat_completion = client.chat.completions.create(\n            messages=[\n                { \"role\": \"system\", \"content\": \"You are an expert of data extraction from a given chunk of text.\"}, # system_role\n                { \"role\": \"user\", \"content\": t+\"\\n\"+p}, # t = text, p = prompt\n            ],\n            model=model_name,\n\n        temperature=0.00001,\n        top_p=1,\n        frequency_penalty=0,\n        presence_penalty=0\n    )\n    response = chat_completion.choices[0].message.content # response from a single chunk for a single prompt\n    return response\n\n# Processing of Text by Claude\ndef claude_function(t, p, claude_key, model_name):\n    \n    client = anthropic.Anthropic(\n        api_key = claude_key,\n    )\n    \n    message = client.messages.create(\n        model = model_name,\n        max_tokens = 1000,\n        temperature = 0.00001,\n        system = \"You are an expert of data extraction from a given chunk of text.\",\n        messages = [\n            {\"role\": \"user\", \"content\": t+\"\\n\"+p}\n    ])\n    \n    msg = message.content\n    return msg","metadata":{"_uuid":"7cf2d891-32dc-48e2-a858-57af637f258e","_cell_guid":"11d7b24c-c680-4541-b48a-7c3562af50a7","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Image Module","metadata":{"_uuid":"305dedbc-ab4e-4f7e-acfc-755443a25a84","_cell_guid":"39fe2cd2-1fef-4f40-bcb7-bcd91dabcf40","trusted":true}},{"cell_type":"code","source":"# save the complete page that contains images/tables as an image\ndef extract_images_from_pdf(pdf_path, image_folder): \n    \n    keywords = [\"Table\", \"Figure\", \"TABLE\", \"FIGURE\", \"table\", \"figure\"]\n    dpi = 500\n    doc = fitz.open(pdf_path)\n\n    for page_num in range(len(doc)):\n        page = doc.load_page(page_num)\n        text = page.get_text()\n\n        # Check if any of the keywords exist in the current page's text\n        if any(keyword in text for keyword in keywords):\n            pix = page.get_pixmap(matrix=fitz.Matrix(dpi / 72, dpi / 72))\n            image_filename = f\"{image_folder}/page_{page_num + 1}_high_res.png\"\n            pix.save(image_filename)\n    \n    doc.close()\n    \ndef encode_image(image_path):\n    with open(image_path, \"rb\") as image_file:\n        return base64.b64encode(image_file.read()).decode('utf-8') # image is decoded into a sutiable format\n\n# This function is used to process image using GPT.\ndef image_processing_gpt(query, image_path, gpt_key):\n    \n    base64_image = encode_image(image_path) # image format conversion\n    \n    # setting of API\n    headers = {\n        \"Content-Type\": \"application/json\",\n        \"Authorization\": f\"Bearer {gpt_key}\"\n    }\n    \n    # here we load query and image \n    payload = {\n        \"model\": \"gpt-4o\",\n        \"messages\": [\n          {\n            \"role\": \"user\",\n            \"content\": [\n              {\n                \"type\": \"text\",\n                \"text\": query\n              },\n              {\n                \"type\": \"image_url\",\n                \"image_url\": {\n                  \"url\": f\"data:image/jpeg;base64,{base64_image}\"\n                }\n              }\n            ]\n          }\n        ],\n        \"max_tokens\": 1000\n    }\n\n    response = requests.post(\"https://api.openai.com/v1/chat/completions\", headers=headers, json=payload) # API call\n    response = response.text\n    if \"content\" in response and len(response) > 50:\n        response = response.split(\"content\")[1][4:] # start from 4th char till 42nd last char\n    if \"finish_reason\" in response and len(response) > 50:\n        response = response.split(\"finish_reason\")[0][:-17] # start from 4th char till 42nd last char\n    \n    return response\n\n# This function is used to process image using Claude.\ndef image_processing_claude(query, image_path, claude_key, model_name):\n    \n    base64_image = encode_image(image_path) # image format conversion\n    \n    client = anthropic.Anthropic(\n        api_key = claude_key,\n    )\n        \n    message = client.messages.create(\n        model = model_name,\n        max_tokens = 1024,\n        messages = [\n            {\n                \"role\": \"user\",\n                \"content\": [\n                    {\n                        \"type\": \"image\",\n                        \"source\": {\n                            \"type\": \"base64\",\n                            \"media_type\": \"image/png\",\n                            \"data\": base64_image,\n                        },\n                    },\n                    {\n                        \"type\": \"text\",\n                        \"text\": query\n                    }\n                ],\n            }\n        ],\n    )\n    return str(message.content)","metadata":{"_uuid":"80835d02-ae2f-49aa-a697-54cf66d46bbd","_cell_guid":"b31b3b8d-f42c-48ea-a4d2-505e6fe9e1be","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Cost Estimation","metadata":{"_uuid":"bb4cd648-d74b-4200-b70e-c4f23eb4f902","_cell_guid":"bbc31c8a-bdbc-49cc-bccc-dd9a61e99333","trusted":true}},{"cell_type":"code","source":"def cost_calculation(input_text, output_text, kf_count, input_cost, output_cost, image_cost):\n    \n    encoding = tiktoken.get_encoding(\"cl100k_base\")\n    \n    #input cost estimation\n    if (len(input_text)) > 1:\n        tokens = encoding.encode(input_text) # encode the text into tokens\n        print (\"Input Tokens:\",len(tokens))\n        chunks = int(len(tokens)/1000) + 1\n        input_cost = input_cost * chunks\n    else:\n        input_cost = 0\n\n    #output cost estimation\n    if (len(output_text)) > 1:\n        tokens = encoding.encode(output_text) # encode the text into tokens\n        print (\"Output Tokens:\",len(tokens))\n        chunks = int(len(tokens)/1000) + 1\n        output_cost = output_cost * chunks\n    else:\n        output_cost = 0\n        \n    #Image cost estimation\n    ext_image_cost = 0\n    if kf_count > 0:\n        print (\"Images:\",kf_count)\n        ext_image_cost = kf_count * image_cost\n    \n    total_cost = input_cost + output_cost + ext_image_cost\n    return  total_cost","metadata":{"_uuid":"d69c864f-f601-4b05-8096-39425f03776e","_cell_guid":"382d4cf2-c819-4d61-8dc7-57e56608f7b5","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Prompt Design","metadata":{"_uuid":"58acdb7a-4878-4b1d-b19a-c4fb6a6f43a3","_cell_guid":"916e989b-0d85-453f-8da8-e091f9c73b28","trusted":true}},{"cell_type":"code","source":"def prompt_design(variables, size):\n\n    splits = int(len(variables)/size) # division of variables into chunks for each prompt\n    start = 0\n    end = size\n    list_of_queries = []\n\n    # for each query, we first define the variables from start_point to end_point (first loop), and then append these variables in the prompt (second loop)\n    for i in range(splits+1):\n        # first case (i = 0): start = 0, end = size\n        if i < splits and i > 0: # intermediate cases (except for the first and last ones)\n            start = end\n            end = end + size\n        elif i == splits and len(variables) % size != 0: # last case\n            start = end\n            end = len(variables)\n            \n        elif i>0: # this is the special case, when % = 0 and i > 0\n            break        \n        \n        query = \"\"\"Extract from the given text, Extract \"\"\"\n        for j in range(start, end): # append variables from start to end in the prompt\n            if j==start:\n                query += \"[\" + variables[j] + \"]\"\n            else:\n                query += \", \" + \"[\" + variables[j] + \"]\"\n        query += \"\"\". Where the information is not available, the output should be “NA”. \n        Do not include information outside the given text. Generate short and precise responses. \"\"\"\n        list_of_queries.append(query) # list of all prompts\n\n    return list_of_queries","metadata":{"_uuid":"a7b54550-7603-4736-9958-86cfb4723632","_cell_guid":"4fff5b2b-e1ca-47b8-9c91-63fe682f5fda","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Apply Prompts (Data Extraction using LLMs)","metadata":{"_uuid":"c4201bf1-cfb6-4330-a517-a6117900926c","_cell_guid":"7bb5c043-6c16-4bf0-94c5-6263689e1041","trusted":true}},{"cell_type":"code","source":"def apply_prompts(folder, model, model_key, variables, size):\n\n    prompts = prompt_design(variables, size) # here we get the prompts with 'size' number of variables\n    print (\"Number of Prompts:\", len(prompts))\n    filenames = os.listdir(folder) # get all filenames from the folder\n    print (\"Number of Files:\", len(filenames)) # verify that you have extracted all the files\n    print ('~'*90)\n    \n    image_foldername = os.getcwd() + \"/PDF_Images/\"\n    os.makedirs(image_foldername, exist_ok=True)\n    \n    pdf_count = image_count = kf_count = 0\n    \n    # Text Extraction\n    input_text = \"\"\n    output_text = \"\"\n    allresponses_list = []\n    \n    if \"gpt\" in model:\n        chunk_start = 124000\n        chunk_end = 123000\n    elif \"claude\" in model: # token size will be adjusted according to usage tier\n        chunk_start = 46000\n        chunk_end = 45000\n    else:\n        chunk_start = 12000\n        chunk_end = 11000\n    \n    for f in filenames:\n        \n        if \".pdf\" in f:\n            \n            pdf_count += 1\n            print (\"\\n\\nFile \", f, \" is in Process ...\\n\")\n            pdffile_path = folder + f # addition of filename with folder path\n            \n            # text extraction from pdf\n            pdftext = extract_text_from_pdf(pdffile_path) # extracting text from pdf            \n            tokens_list = get_tokens(pdftext, \"cl100k_base\", chunk_start, chunk_end) # comment this line for previous version\n            \n            encoding = tiktoken.get_encoding(\"cl100k_base\")\n            tokens2 = encoding.encode(pdftext) # encode the text into tokens\n            \n            print (\"Extracted Tokens:\", len(tokens2),\"\\nExtracted Chunks:\", len(tokens_list),\"\\n\")\n            \n            allresponses = \"FILENAME: \"+f+\"\\n\"\n            allresponses += \"TEXTRESPONSES\"\n            for t in tokens_list: # iterate over all the chunks of a document f\n                response_text = \"\"\n                for p in prompts: # iterate over all sub-prompts\n                    \n                    if \"claude\" in model:\n                        response = claude_function(t, p, model_key, model)\n                        response = str(response)\n                        time.sleep(30) # to avoid TPM limit\n                    else:\n                        response = gpt_function(t, p, model_key, model)\n                        \n                    response_text = response_text + \"\\n\" + response # concatenate responces of all prompts for a single chunk of text\n                    \n                    input_text = input_text + \"\\n\" + t + \"\\n\" + p\n                    output_text = output_text + \"\\n\" + response\n                    \n                allresponses = allresponses + \"\\n\" + response_text # response_text = output of one chunk\n            \n            # image extraction from pdf\n            os.makedirs(image_foldername + f, exist_ok=True) # sub_directory to store the images of a particular pdf\n            extract_images_from_pdf(pdffile_path, image_foldername + f) # function call to extract images from a pdf_file\n\n            images = os.listdir(image_foldername + f) # get names of all images from the folder\n            print (\"Extracted Images:\",len(images))\n            allresponses = allresponses + \"\\n\\n\" + \"IMAGERESPONSES\"\n            for img in images:\n\n                image_count += 1 # Timer on Images for ChatGPT\n                print (\" - Images are in Process...\")\n                    \n                image_path = image_foldername + f + \"/\" + img # get image path    \n                img_response = \"\"\n                \n                for p in prompts: # iterate over all sub-prompts\n                    \n                    if \"claude\" in model:\n                        image_result = image_processing_claude(p, image_path, model_key, model)\n                        image_result = str(image_result)\n                        if image_count % 3 == 0:\n                            time.sleep(30) # to avoid TPM limit\n                    else:\n                        image_result = image_processing_gpt(p, image_path, model_key) # call GPT here\n                        if image_count % 12 == 0:\n                            time.sleep(60) # to avoid TPM limit\n                        \n                    img_response = img_response + \"\\n\" + image_result # concatenate responces of all prompts for a single chunk of text\n                    output_text = output_text + \"\\n\" + image_result\n                    \n                allresponses = allresponses + \"\\n\" + img_response # response_text = output of one chunk\n                \n            allresponses_list.append(allresponses) # allresponses = output of one pdf\n            \n    kf_count = kf_count + (image_count*len(prompts))\n    return allresponses_list, input_text, output_text, kf_count # output of all pdfs","metadata":{"_uuid":"b020d148-5977-474f-b0b9-f29d64c62a60","_cell_guid":"7066f153-1141-4e46-ae5d-d00bb366e281","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### I/O File","metadata":{}},{"cell_type":"code","source":"# Read/Write Raw Responses from the File\ndef read_raw_responses(path):\n    \n    f = open(path)\n    data = f.readlines()\n    f.close()\n\n    strings = \"\"\n    results = []\n    for i in data:\n        if \"###\" not in i:\n            strings += i\n        elif \"###\" in i and len(i) < 5:\n            results.append(strings)\n            strings = \"\"\n            \n    return results\n\ndef write_raw_responses(results, response_recording):\n    \n    f = open(response_recording,'w')\n        \n    for result in results:\n        r = result.split(\"\\n\")\n        for i in r:\n            f.writelines(i+\"\\n\")\n        f.writelines(\"\\n###\\n\")\n        \n    f.close()","metadata":{"_uuid":"322a0d8c-8fdd-444f-a538-cac3662cde85","_cell_guid":"ff8be4c8-74f0-47af-bd04-c9baa1cb3cc1","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Post Processing","metadata":{"_uuid":"17675a03-c865-4895-89f3-5b209cdade5e","_cell_guid":"87f34218-7ee7-433e-914c-6577956f86fb","trusted":true}},{"cell_type":"code","source":"def post_processing(path, model_key, model):\n    \n    results = read_raw_responses(path) # read the raw_response from the file\n    \n    pp_query = \"\"\"As an expert in processing the text in a structured format, identify:\n    1.Name of the clinical trial\n    2.Names of variables (keep the variable name short and consistent)\n    3.Value of each variable\n    Present the outcome as:\n    \n    Example OUTPUT (tabular format):\n    Variable 1 name :: Variable value\n    Variable 2 name :: Variable value\n    ...\n    \n    4. Generate the output as concise as possible for all 23 variables. Also include the variables for which value is NA.\n    5. Do not include any other information including extra spaces, numbering and special character.\"\"\"\n    \n    pp_resps = []\n    for result in results:\n        if \"gpt\" in model:\n            pp_resp = gpt_function(result, pp_query, model_key, model)\n        else:\n            pp_resp = claude_function(result, pp_query, model_key, model)\n            pp_resp = str(pp_resp)\n        pp_resps.append(pp_resp)\n        print (pp_resp,\"\\n\")\n            \n    return pp_resps","metadata":{"_uuid":"d88a4dc1-9245-406a-9788-a6d4138dd3f0","_cell_guid":"223bcd33-2e33-444b-8540-8e67d0b98100","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Extraction","metadata":{"_uuid":"07d9b6cd-b806-4c27-bd30-b2c5622e2d55","_cell_guid":"6f383c92-1a71-4e8c-8c4c-bf257c8cc6f3","trusted":true}},{"cell_type":"code","source":"def extract_using_llms(model, model_key, prompt_size, pdf_folder_path, variable_file_path):\n    \n    dd = pd.read_csv(variable_file_path)\n    variables = list(dd.loc[0])\n    \n    pdf_folder_path = pdf_folder_path + \"/\"\n                    \n    print ('~'*90)\n    print (\"LLMs are in Process to Generate Responses\")\n    print (\"Model:\", model, \"\\tPrompt Size:\", prompt_size, \"\\t# of Variables:\", len(variables))\n    print ('~'*90)\n\n    results, input_text, output_text, kf_count = apply_prompts(pdf_folder_path, model, model_key, variables, prompt_size)\n\n    t = time.localtime()\n    current_time = time.strftime(\"%H:%M:%S\", t)\n\n    output_path = os.getcwd()\n    write_path = output_path + \"/\" + model + \"__\" + current_time + \".txt\"\n    write_raw_responses(results, write_path)\n    print (\"\\nRaw Responses are Recorded in File:\", write_path)\n\n    print (\"\\n\\nPost Processed Results\\n\")\n    pp_resp = post_processing(write_path, model_key, model)\n\n    input_text += str(results)\n    output_text += str(pp_resp)\n\n    t = time.localtime()\n    current_time = time.strftime(\"%H:%M:%S\", t)\n    write_path = output_path + \"/\" + model + \"_PP\" + \"__\" + current_time + \".txt\"\n    write_raw_responses(pp_resp, write_path)\n    print (\"\\nPost Processed Responses are Recorded in File:\", write_path)\n\n    if \"gpt\" in model:\n        cost = cost_calculation(input_text, output_text, kf_count, 0.0025, 0.010, 0.001)\n        print (\"\\nTotal Cost of GPT-04:\", cost)\n    else:\n        cost = cost_calculation(input_text, output_text, kf_count, 0.005, 0.015, 0.0025)\n        print (\"\\nTotal Cost of Claude:\", round(cost,4))\n\ndef extract_from_files(raw_resp_file, pp_resp_file):\n\n    print (\"Reading Recorded Raw Responses\")\n    results = read_raw_responses(raw_resp_path)\n\n    for res in results:\n        print (res,\"\\n\")\n\n    print (\"Reading Recorded Post-Processed Responses\")\n    results = read_raw_responses(pp_resp_path)\n\n    for res in results:\n        toks = res.split(\"\\n\")\n        for tok in toks:\n            print (tok)","metadata":{"_uuid":"e23a6311-de44-4b46-9f2c-e20af452cf67","_cell_guid":"4ed82f8c-f96d-4a3f-aba2-300b66f39ea0","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"### For Jupyter Notebook Execution ###\nchoice = input(\"Enter the Choice for LLMs Execution (Y/N): \")\nif choice == 'Y' or choice == 'y':\n    ### INPUT ###\n    pdf_folder = input(\"Enter the Path to PDF Folder: \") # \"/kaggle/input/extractiontrain\"\n    variable_file = input(\"Enter the Path to the Variable file: \") #\"/kaggle/input/pdftest/variables.csv\"\n    model = input(\"Enter the Model Name: \") # claude-3-opus-20240229 (sonnet models can also be used) / gpt-4-0125-preview (gpt-4o can also be used, tested)\n    model_key = input(\"Enter the Key for the model: \") # -- it will be different for claude/gpt\n    prompt_size = int(input(\"Enter the number of variables for each prompt (3, 5, 10, ...): \"))\n    print (\"\\n\")\n    ### LLM_CALL ###\n    extract_using_llms(model, model_key, prompt_size, pdf_folder, variable_file)\n\nelse:\n    # reading recorded responses\n    raw_resp_file = input(\"Enter the Path to the Response File: \") #\"/kaggle/input/pdftest/gpt-4-0125-preview__19_04_31.txt\"\n    pp_resp_file = input(\"Enter the Path to the Processed-Response File: \") #\"/kaggle/input/pdftest/gpt-4-0125-preview_PP__19_04_38.txt\"\n    extract_from_files(raw_resp_file, pp_resp_file)","metadata":{"_uuid":"b60c1be6-cca1-44b9-acd0-bc2c231eb2eb","_cell_guid":"57c4d47e-7fe8-4be1-b259-5fd9e70e86f6","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Dis-Agreement Resolution","metadata":{"_uuid":"08850b2f-b5d7-49a2-823b-b118592e399d","_cell_guid":"015d6ded-8291-4b0f-bc70-a6c0dce7fc19","trusted":true}},{"cell_type":"code","source":"def disagreement_resolution(test_folder, model, model_key, agreement_filepath):\n\n    files_in_folder = os.listdir(test_folder)\n    df = pd.read_excel(agreement_filepath)\n\n    variables = df[\"Variable\"]\n    gr = df[\"GPT_Responses\"]\n    cr = df[\"CLAUDE_Responses\"]\n    gs = df[\"Gold_Standard\"]\n    ad = df[\"Agree(A)/Disagree(D)\"]\n    \n    files = df[\"File\"]\n    for i in range(files.shape[0]):\n        # files[i] = files[i].replace(\"'\",\"\")\n        if \".pdf\" not in files[i]:\n            files[i] += \".pdf\"        \n    files = files.values.tolist()\n\n    chunk_start = 46000\n    chunk_end = 45000\n\n    allresponses_list = []\n\n    os.makedirs(\"Images_AD\", exist_ok=True) # create directory to store images\n    image_foldername = os.getcwd() + \"/Images_AD/\" # super-folder path where images of each file will be stored into sub-folders\n    count=0\n    \n    for i in range(len(cr)):\n\n        if ad[i] == \"D\": # Disagreement Check\n            file = files[i]    # get the filename\n            if file in files_in_folder: # file exist in folder\n                print (file, \"\\t\" ,count, \"\\n\")\n                count+=1\n\n                # Prompt Design\n                var = str(variables[i])\n                if model == \"claude\":\n                    val = str(gr[i])\n                else:\n                    val = str(cr[i])\n                p = \"For the given text, LLM generate the response = [\" + val + \"] for the variable = [\" + var + \"]. Verify if the response for the given variable generated by LLM is correct or incorrect. If the response is incorrect, then generate the correct response (as short and precise as possible).\"\n\n                pdffile_path = test_folder + \"/\" + file\n\n                pdftext = extract_text_from_pdf(pdffile_path) # extracting text from pdf            \n                tokens_list = get_tokens(pdftext, \"cl100k_base\", chunk_start, chunk_end) # comment this line for previous version\n\n                encoding = tiktoken.get_encoding(\"cl100k_base\")\n                tokens2 = encoding.encode(pdftext) # encode the text into tokens\n\n                print (\"Extracted Tokens:\", len(tokens2),\"\\nExtracted Chunks:\", len(tokens_list),\"\\n\")\n\n                allresponses = \"File Name: \" + file + \"\\n\"\n                allresponses += \"Variable Name: \" + var + \"\\n\"\n                allresponses += \"Gold Standard: \" + str(gs[i]) + \"\\n\"\n                allresponses += \"GPT Response: \" + str(gr[i]) + \"\\n\"\n                allresponses += \"Claude Response: \" + str(cr[i]) + \"\\n\\n\"\n                allresponses += \"Verification from the Text...\\n\"\n\n                for t in tokens_list: # iterate over all the chunks of a document f\n\n                    if model == \"claude\":\n                        response = claude_function(t, p, model_key, model)\n                        response = str(response)\n                        time.sleep(30) # to avoid TPM limit\n                    else:\n                        response = gpt_function(t, p, model_key, model)\n\n                    allresponses = allresponses + \"\\n\" + response # concatenate responces of all prompts for a single chunk of text\n\n\n                # image extraction from pdf\n                os.makedirs(image_foldername + file, exist_ok=True) # sub_directory to store the images of a particular pdf\n                extract_images_from_pdf_2(pdffile_path, image_foldername + file) # function call to extract images from a pdf_file\n\n                images = os.listdir(image_foldername + file) # get names of all images from the folder\n                print (\"Extracted Images:\",len(images))\n                allresponses = allresponses + \"\\n\\n\" + \"Verification from the images...\"\n\n                for img in images:\n\n                    print (\" - Images are in Process...\")\n\n                    image_path = image_foldername + file + \"/\" + img # get image path    \n\n                    if \"claude\" in model:\n                        image_result = image_processing_claude(p, image_path, model_key, model)\n                        image_result = str(image_result)\n                        time.sleep(20) # to avoid TPM limit\n                    else:\n                        image_result = image_processing(p, image_path, model_key) # call GPT here\n                        time.sleep(10)\n\n                    allresponses = allresponses + \"\\n\\n\" + image_result # concatenate responces of all prompts for a single chunk of text\n                print (\"\\n\")\n\n                allresponses_list.append(allresponses) # allresponses = output of one pdf\n\n    ff = \"DisagreementResolution_by_\"+model            \n    write_raw_responses(allresponses_list, ff) # change file name for claude","metadata":{"_uuid":"435b7915-b11c-440f-b103-d1adbfb86fde","_cell_guid":"a7852578-fbe9-4f5d-9c1c-7b244de83685","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"a = input(\"Do you to execute Disagreement Resolution Module? Y/N\")\nif a == \"Y\" or a == \"y\":\n    model = input(\"Enter the Model Name\") # or claude-3-opus-20240229 or gpt-4-0125-preview\n    key = input(\"Enter the Key\")\n    test_folder = input(\"Enter the Path to the PDF Folder\") # \"/kaggle/input/extractiontrain\"\n    agreement_filepath = input(\"Enter the Path to the Annotated Agreement Matching File\") # \"/kaggle/input/pdftest/TRAIN_GPT_Claude_agreement.xlsx\"    \n    disagreement_resolution(test_folder, model, model_key, agreement_filepath)\nelse:\n    print (\"You Choose Not to Execute Agreement/Disagreement Module.\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* ***Graphical representation of the Results***","metadata":{"_uuid":"a2261b80-858f-4917-bc56-e3c8d64d3d37","_cell_guid":"b38bcd8a-5341-4456-ab19-5fe666194fe6","trusted":true}},{"cell_type":"code","source":"# # data from https://allisonhorst.github.io/palmerpenguins/\n\n# import matplotlib.pyplot as plt\n# import numpy as np\n\n# species = (\"Baseline Prompts\", \"Eng. Prompts\")\n# penguin_means = {\n#     'GPT-3.5': (55, 60),\n#     'GPT-4': (62, 68),\n# }\n\n# x = np.arange(len(species))  # the label locations\n# width = 0.25  # the width of the bars\n# multiplier = 0\n\n# fig, ax = plt.subplots(layout='constrained')\n\n# for attribute, measurement in penguin_means.items():\n#     offset = width * multiplier\n#     rects = ax.bar(x + offset, measurement, width, label=attribute)\n#     ax.bar_label(rects, padding=2)\n#     multiplier += 1\n\n# # Add some text for labels, title and custom x-axis tick labels, etc.\n# ax.set_ylabel('Performance')\n# ax.set_title('Performance Comparison of GPT Models')\n# ax.set_xticks(x + width - 0.13, species)\n# ax.legend(loc='upper left', ncols=2)\n# ax.set_ylim(0, 100)\n\n# plt.show()","metadata":{"_uuid":"d8ed183e-ac5a-436f-bac7-d20220ffa5fb","_cell_guid":"5df02fa4-0de5-47a5-8a9b-4c3dc835c9af","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* ***Random Sampling***","metadata":{"_uuid":"e017ccd1-c9bd-465b-a63e-850213329540","_cell_guid":"dcb54f2f-0c97-420d-9d2a-71839420b14f","trusted":true}},{"cell_type":"code","source":"# filenames = os.listdir(\"/kaggle/input/extraction/\") # get all filenames from the folder\n# print (\"Number of Files:\", len(filenames)) # verify that you have extracted all the files\n\n# files = []\n# for file in filenames:\n#     if \".pdf\" in file and \"SLIDE\" not in file:\n#         files.append(file)\n        \n# print (len(files))\n\n# training_files = random.sample(files, 5)\n# for file in training_files:\n#     print (file)","metadata":{"_uuid":"59a55213-d3bd-4757-b00b-fa39800e3996","_cell_guid":"a6ceb54b-3d4f-406a-b634-db3ece96719b","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]}]}